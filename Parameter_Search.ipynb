{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8biuEdpmKoh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, functions and classes"
      ],
      "metadata": {
        "id": "ipgaw8VwPfAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "GkXRKd8HrFcG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3123909-6f2a-41b9-8545-28d6290350d7",
        "outputId": "1615ac15-107e-4ffd-80b0-a37597fb8468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "from datetime import datetime\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import itertools\n",
        "import pandas as pd\n",
        "!pip install -q torchsummary\n",
        "from torchsummary import summary\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USbGLTxvdQdA",
        "outputId": "426da4e1-7423-4965-e7a1-199ce75a6623"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "opRUT0cJ0i_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adder dataloader\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "General adder to define dataloader automatically based on Nbits"
      ],
      "metadata": {
        "id": "6j4KlWvvrepV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def NAND(x, y):\n",
        "     if x == 0 and y == 0: return 1\n",
        "     if x == 0 and y == 1: return 1\n",
        "     if x == 1 and y == 0: return 1\n",
        "     if x == 1 and y == 1: return 0\n",
        "\n",
        "def NOT(x):\n",
        "     return NAND(x, x)\n",
        "\n",
        "def AND(x, y):\n",
        "     return NOT(NAND(x, y))\n",
        "\n",
        "def OR(x, y):\n",
        "     return NAND(NAND(x, x), NAND(y, y))\n",
        "\n",
        "def XOR(x, y):\n",
        "     return AND(OR(x, y),\n",
        "                NOT(AND(x, y)))\n",
        "\n",
        "def HALF(x, y):\n",
        "     carry = AND(x, y)\n",
        "     sum = XOR(x, y)\n",
        "     return (carry, sum)\n",
        "\n",
        "def FULL(x, y, carry_in):\n",
        "     carry1, sum1 = HALF(x, y)\n",
        "     carry2, sum2 = HALF(carry_in, sum1)\n",
        "     carry_out = OR(carry1, carry2)\n",
        "     return (carry_out, sum2)\n",
        "\n",
        "def ADDN(left, right, carry_in):\n",
        "    N = len(left)\n",
        "    sums = []\n",
        "    carry = carry_in\n",
        "\n",
        "    for i in range(N-1, -1, -1):\n",
        "        carry, sum_ = FULL(left[i], right[i], carry)\n",
        "        sums.insert(0, sum_)\n",
        "\n",
        "    return sums + [carry]\n",
        "\n",
        "\n",
        "def generate_adder_dataframe(N, add_constant=True, constant_value=1):\n",
        "    inputs = [\n",
        "        [*bin(i)[2:].zfill(N), *bin(j)[2:].zfill(N), carry_in]\n",
        "        for i in range(2**N)\n",
        "        for j in range(2**N)\n",
        "        for carry_in in range(2)\n",
        "    ]\n",
        "\n",
        "    inputs = [[int(x) for x in row] for row in inputs]\n",
        "\n",
        "    outputs = [ADDN(row[:N], row[N:2*N], row[2*N]) for row in inputs]\n",
        "\n",
        "    in_columns = [f\"a{i}\" for i in range(1, N+1)] + [f\"b{i}\" for i in range(1, N+1)] + [\"carry_in\"]\n",
        "    out_columns = [f\"sum{i}\" for i in range(1, N+1)] + [\"carry_out\"]\n",
        "\n",
        "    in_df = pd.DataFrame(inputs, columns=in_columns)\n",
        "    if add_constant:\n",
        "      in_df[\"Constant\"] = constant_value\n",
        "    out_df = pd.DataFrame(outputs, columns=out_columns)\n",
        "    return pd.concat([in_df, out_df], axis=1), len(in_df.columns), len(out_df.columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "3582GaEMtd5d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdderDataset(Dataset):\n",
        "    def __init__(self, data, n_inputs):\n",
        "        self.data = data\n",
        "        self.n_inputs=n_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_data = torch.tensor(self.data.iloc[idx, :self.n_inputs].values, dtype=torch.float32)\n",
        "        target_data = torch.tensor(self.data.iloc[idx, self.n_inputs:].values, dtype=torch.float32)\n",
        "        return input_data, target_data\n",
        "\n"
      ],
      "metadata": {
        "id": "E2Ey6WH1tvf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise Function\n",
        "Considering the noise as Guassian noise"
      ],
      "metadata": {
        "id": "TrXZnbbJ_2Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filepath):\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    #print(f\"Model saved to {filepath}\")\n",
        "\n",
        "def load_model(model, filepath):\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    model.to(device)\n",
        "    #print(f\"Model loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "75i8u644fIKl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantized Linear function\n",
        "**round_to_nearest_values**: Approximate weights to predefined quantization levels for Quantization-Aware Training (QAT).\n",
        "\n",
        "---\n",
        "\n",
        "**QuantizedLinear**: A class that functions as a quantized fully connected (FC) layer when `PTQ` is `True`, and as a standard FC layer when `PTQ` is `False`.\n"
      ],
      "metadata": {
        "id": "qWaAepCU0-5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def round_to_nearest_values(tensor, values):\n",
        "    tensor.to(device)\n",
        "    values_tensor = torch.tensor(values, dtype=torch.float32).unsqueeze(0).to(device)  # Convert values list to tensor and add a batch dimension\n",
        "    diff = torch.abs(tensor.unsqueeze(-1) - values_tensor)  # Calculate absolute differences\n",
        "    min_indices = torch.argmin(diff, dim=-1)  # Find indices of minimum differences\n",
        "    return values_tensor[0, min_indices]  # Gather the nearest values\n",
        "\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias=None, s=1.0):\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        ctx.s = s\n",
        "\n",
        "        # Scale the weights\n",
        "        scaled_weight = weight / s\n",
        "\n",
        "        weight_q=round_to_nearest_values(scaled_weight, possible_weights)\n",
        "\n",
        "        if bias is not None:\n",
        "            # Apply similar logic to bias if necessary\n",
        "            scaled_bias = bias / s\n",
        "            bias_q = torch.sign(scaled_bias) * torch.where(torch.abs(scaled_bias) >= 1.5, 2,\n",
        "                                                           torch.where(torch.abs(scaled_bias) < 1.5, 1, 0))\n",
        "            output = F.linear(input, weight_q * s, bias_q * s)\n",
        "        else:\n",
        "            output = F.linear(input, weight_q * s)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, weight, bias = ctx.saved_tensors\n",
        "        s = ctx.s\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "            grad_weight = grad_weight / s\n",
        "        if bias is not None and ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "            grad_bias = grad_bias / s\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, None\n",
        "###########################################################################################################\n",
        "# PTQ is a flag that if True simulates quantization with the possible weights and if False works as a standard FC layer\n",
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=False, s=1.0, PTQ=False):\n",
        "        super(QuantizedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.s = s\n",
        "        self.reset_parameters()\n",
        "        self.PTQ=PTQ\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.PTQ:\n",
        "          return STEFunction.apply(input, self.weight, self.bias, self.s)\n",
        "        else:\n",
        "          if self.bias is not None:\n",
        "             return F.linear(input,self.weight *self.s, self.bias*self.s)\n",
        "          else:\n",
        "            return F.linear(input, self.weight*self.s)\n"
      ],
      "metadata": {
        "id": "XTclqolS1BD0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "**BinaryStep** activation and **MLP** model. The goal is to use a BinaryStep function with a threshold of 0.5 as the activation layer in this case. During the backward pass, we simulate a sigmoid function to facilitate better learning for the model. The MLP model has a parameter called `STEP` that determines the activation function used between layers. If `STEP` is `True`, the BinaryStep function is used as the activation function. If `STEP` is `False`, the sigmoid function is used as the activation function."
      ],
      "metadata": {
        "id": "eRT_7wux1-IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryStep(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = torch.where(input >= 0.5, torch.tensor(1.), torch.tensor(0.))\n",
        "        ctx.save_for_backward(input)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "\n",
        "        # Calculate sigmoid derivative\n",
        "        sigmoid_derivative = torch.sigmoid(input) * (1 - torch.sigmoid(input))\n",
        "        grad_input *= sigmoid_derivative  # Element-wise multiplication with the sigmoid derivative\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_feat, num_classes, units, s=1.0, STEP=False):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Input layer\n",
        "        layers.append(QuantizedLinear(in_feat, units[0], s=s))\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(units)):\n",
        "            layers.append(QuantizedLinear(units[i-1], units[i], s=s))\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(QuantizedLinear(units[-1], num_classes, s=s))\n",
        "\n",
        "        # Store layers as a ModuleList\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "        self.act_qt = BinaryStep()\n",
        "        self.act_sig = nn.Sigmoid()\n",
        "        # If STEP is True, use BinaryStep activation layer with threshold on 0.5. If False, use sigmoid layer\n",
        "        self.STEP = STEP\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x)\n",
        "            x = self.act_qt.apply(x) if self.STEP else self.act_sig(x)\n",
        "\n",
        "        # Handle the last layer separately to apply sigmoid activation\n",
        "        x = self.layers[-1](x)\n",
        "        return self.act_qt.apply(x) if self.STEP else torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "Wby1ZOkT1_ba"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Optional WarmupScheduler\n",
        "class WarmupScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_epochs, base_lr, final_lr, after_scheduler):\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.base_lr = base_lr\n",
        "        self.final_lr = final_lr\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished_warmup = False\n",
        "        super(WarmupScheduler, self).__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if not self.finished_warmup:\n",
        "            current_epoch = self.last_epoch + 1\n",
        "            if current_epoch <= self.warmup_epochs:\n",
        "                lr = self.base_lr + (self.final_lr - self.base_lr) * current_epoch / self.warmup_epochs\n",
        "                return [lr for _ in self.base_lrs]\n",
        "            else:\n",
        "                self.finished_warmup = True\n",
        "                self.after_scheduler.base_lrs = [self.final_lr for _ in self.base_lrs]\n",
        "        return self.after_scheduler.get_lr()\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if not self.finished_warmup:\n",
        "            super(WarmupScheduler, self).step(epoch)\n",
        "        else:\n",
        "            if epoch is not None:\n",
        "                self.after_scheduler.step(epoch - self.warmup_epochs)\n",
        "            else:\n",
        "                self.after_scheduler.step()\n"
      ],
      "metadata": {
        "id": "SEi6Ag-U87iQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for training\n",
        "We use `clip_weights` after each epoch to ensure the weights remain close to the desired discrete values."
      ],
      "metadata": {
        "id": "UQ1hAbku5vHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_weights(model, s=1.0):\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            # Determine if we're dealing with weights or biases\n",
        "            if 'weight' in name or 'bias' in name:\n",
        "                # Scale the parameter\n",
        "                scaled_param = param / s\n",
        "                quantized_param =round_to_nearest_values(scaled_param, possible_weights)\n",
        "                # Scale back and update the parameter\n",
        "                param.copy_(quantized_param * s)\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    # Convert probabilities to binary predictions\n",
        "    predicted = y_pred > 0.5\n",
        "    # Compare predictions to true values\n",
        "\n",
        "    correct = (predicted == y_true).float()  # Convert boolean to float for calculation\n",
        "    accuracy = correct.mean()  # Calculate mean accuracy per example\n",
        "    return accuracy\n",
        "\n",
        "def clip_weights(model, min_value, max_value):\n",
        "    for param in model.parameters():\n",
        "        param.data.clamp_(min_value, max_value)\n",
        "\n",
        "def evaluate_model(train_loader):\n",
        "    model.eval()\n",
        "    total_accuracy = 0\n",
        "    for x, y in train_loader:\n",
        "        print('x=', x)\n",
        "        print('y=', y)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        # print('output', output)\n",
        "        print('-----------')\n",
        "\n",
        "        total_accuracy += calculate_accuracy(y, output).item() / len(train_loader)\n",
        "\n",
        "    print(f'Model Accuracy: {total_accuracy*100}%')\n",
        "    return total_accuracy\n",
        "\n",
        "def get_weights(model):\n",
        "  w_dict={}\n",
        "  for name, param in model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          w_dict[name]=param.data.cpu().numpy().tolist()\n",
        "  return w_dict"
      ],
      "metadata": {
        "id": "WUkTcsOv5y_4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main pipeline"
      ],
      "metadata": {
        "id": "urT6sDyxPyQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset\n",
        "Simply define the number of bits **Nbits**, specify whether a constant column (**add_constant**) should be added, and provide the value for the constant column if needed (**constant_value**). The dataset will then be created automatically."
      ],
      "metadata": {
        "id": "Y8wjA3G90Eyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf666e1-506f-46c4-b5e5-c88f7ce9d651",
        "outputId": "69d4368f-461b-4ac8-df35-987fc36b3067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 bits adder\n",
            "Batch_size: 8\n",
            "Data samples: 8\n"
          ]
        }
      ],
      "source": [
        "Nbits=1\n",
        "add_constant=True\n",
        "constant_value=-1\n",
        "\n",
        "###################################################################################################\n",
        "batch_size = 2**(2*Nbits+1) if Nbits<3 else int(2**(2*Nbits+1)/2)\n",
        "adderDf, n_inputs, n_outs = generate_adder_dataframe(N=Nbits, add_constant=add_constant, constant_value=constant_value)\n",
        "#################################################################################\n",
        "dataset = AdderDataset(adderDf, n_inputs)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "print(f\"{Nbits} bits adder\")\n",
        "print(f\"Batch_size: {batch_size}\")\n",
        "print(f'Data samples: {len(dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adderDf.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "ZZZJkUWE8lgd",
        "outputId": "a3190584-65a9-4523-d017-321c376a9cb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   a1  b1  carry_in  Constant  sum1  carry_out\n",
              "0   0   0         0        -1     0          0\n",
              "1   0   0         1        -1     1          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1117f0d4-8c91-4263-8ad7-32b915fe0a2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a1</th>\n",
              "      <th>b1</th>\n",
              "      <th>carry_in</th>\n",
              "      <th>Constant</th>\n",
              "      <th>sum1</th>\n",
              "      <th>carry_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1117f0d4-8c91-4263-8ad7-32b915fe0a2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1117f0d4-8c91-4263-8ad7-32b915fe0a2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1117f0d4-8c91-4263-8ad7-32b915fe0a2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec3745e8-4e9b-4976-90f1-75ee93a969d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec3745e8-4e9b-4976-90f1-75ee93a969d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec3745e8-4e9b-4976-90f1-75ee93a969d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "adderDf",
              "summary": "{\n  \"name\": \"adderDf\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"a1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"b1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"carry_in\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Constant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": -1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"carry_out\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training parameters\n",
        "\n",
        "The primary goal is to train the model using Quantization Aware Training (QAT) across multiple stages.\n",
        "\n",
        "- **Stage 0**: The weights are quantized, and the layers use the Sigmoid activation function for two epochs, followed by the BinaryStep activation function for one epoch. This alternation helps ease the training process and guide the model towards a weight distribution close to the desired values, aiming for 100% accuracy.\n",
        "- **Stage 1**: The model alternates between Sigmoid and BinaryStep activations every epoch until it achieves 100% accuracy.\n",
        "- **Stage 2**: The BinaryStep activation is used more frequently than Sigmoid.\n",
        "- **Stage 3**: Only BinaryStep activation is used.\n",
        "\n",
        "We also use a cosine scheduler and clip the weights after each epoch to facilitate convergence. If the model does not converge, you may need to increase the number of units or adjust the learning rate.\n",
        "\n",
        "The weights of the best-performing model are saved automatically. If the training gets stuck at any stage, you can stop the process, load the best weights so far, modify the initial stage, and reduce the learning rate to prevent the weights from being destabilized.\n"
      ],
      "metadata": {
        "id": "sTv_J5ec9qgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "\n",
        "After the model finishes training, a JSON file is automatically saved with the date of the training as its name. This file contains essential information about the dataset, hyperparameters, and the size of the model. Additionally, it includes the quantized weights and the achieved accuracy."
      ],
      "metadata": {
        "id": "HEBZOUN5Gp3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd0f97b7-b653-4525-9d20-881c7c8f4023",
        "scrolled": true,
        "outputId": "7730881b-8cb9-4da0-ce9b-cf33766f8bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 th Iteration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e02185d1a7a8>:37: DeprecationWarning: <class '__main__.BinaryStep'> should not be instantiated. Methods on autograd functionsare all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n",
            "  self.act_qt = BinaryStep()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "Epoch 500, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.6875\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:855: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 155000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 155500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 156000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 156500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 157000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 157500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 158000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 158500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 159000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 159500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 160000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 160500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 161000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 161500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 162000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 162500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 163000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 163500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 164000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 164500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 165000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 165500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 166000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 166500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 167000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 167500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 168000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 168500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 169000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 169500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 170000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 170500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 171000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 171500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 172000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 172500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 173000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 173500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 174000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 174500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 175000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 175500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 176000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 176500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 177000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 177500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 178000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 178500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 179000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 179500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 180000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 180500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 181000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 181500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 182000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 182500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 183000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 183500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 184000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 184500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 185000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 185500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 186000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 186500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 187000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 187500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 188000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 188500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 189000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 189500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 190000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 190500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 191000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 191500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 192000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 192500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 193000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 193500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 194000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 194500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 195000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 195500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 196000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 196500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 197000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 197500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 198000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 198500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 199000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 199500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 200000, Loss: 31.2500, Accuracy: 0.6875\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 50.0%\n",
            "x= tensor([[ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 50.0%\n",
            "21 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  1\n",
            "0.875\n",
            "1.0\n",
            "stage:  2\n",
            "0.875\n",
            "1.0\n",
            "stage:  3\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 4000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 4500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 5000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 6500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 7000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 24000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 26500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 39000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 39500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 44500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 47000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 47500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 52000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 53000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 53500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 65500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 67000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 73500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 77000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 77500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 78500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 79000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 79500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 80000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 81500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 97000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 97500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 99000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 99500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 102500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 107500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 112000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 113500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 117500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 125000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 125500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 129500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 130000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 134500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 136000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 138000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 140500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 143000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 144500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 145000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 145500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 146000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 154000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 154500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 157500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 158000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 160000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 161500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 162500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 163000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 163500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 164000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 165000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 166000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 167000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 170000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 170500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 173500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 174000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 176000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 176500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 179000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 180500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 182500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 184500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 185500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 189000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 189500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 193500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 194500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 196000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 198000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 198500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 199000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 200000, Loss: 18.7500, Accuracy: 0.8125\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "22 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 3500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 4000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 4500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "0.5625\n",
            "1.0\n",
            "stage:  2\n",
            "0.5625\n",
            "1.0\n",
            "stage:  3\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 21500, Loss: 25.0000, Accuracy: 0.7500\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 25000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 29000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 33500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 34000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 34500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44000, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[-4.0, 2.0, -4.0, -4.0], [-2.0, 4.0, -2.0, -4.0], [-4.0, 4.0, -4.0, -2.0], [-4.0, -4.0, -4.0, -2.0], [4.0, 4.0, 4.0, 4.0]], 'layers.1.weight': [[0.0, 4.0, -2.0, -2.0, -2.0], [0.0, 0.0, 0.0, -2.0, 2.0]]}\n",
            "23 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.625\n",
            "0.6875\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.75\n",
            "0.9375\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 5000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "1.0\n",
            "stage:  3\n",
            "0.75\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 18500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 20000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[-2.0, -2.0, 2.0, -2.0], [4.0, 2.0, 4.0, 4.0], [-4.0, -2.0, -4.0, -4.0], [-4.0, 2.0, -4.0, -4.0], [2.0, 4.0, 2.0, 0.0]], 'layers.1.weight': [[-2.0, -2.0, 2.0, -2.0, 4.0], [0.0, 2.0, -2.0, 0.0, 0.0]]}\n",
            "24 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.625\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.6875\n",
            "0.75\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.8125\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.875\n",
            "Epoch 2000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 12.5000, Accuracy: 0.8750\n",
            "0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 4500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 13500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43500, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  3\n",
            "0.75\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[4.0, -4.0, -4.0, -2.0], [2.0, 4.0, 4.0, 4.0], [-4.0, 0.0, 0.0, -2.0], [4.0, 0.0, 0.0, -2.0], [2.0, 0.0, 0.0, -2.0]], 'layers.1.weight': [[-2.0, -2.0, -2.0, 2.0, 2.0], [0.0, 2.0, 0.0, 0.0, 0.0]]}\n",
            "25 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 1000, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 3000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.8125\n",
            "1.0\n",
            "stage:  3\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 3500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 5500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 6000, Loss: 31.2500, Accuracy: 0.6875\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[-4.0, 0.0, 0.0, -2.0], [-2.0, 4.0, 4.0, 2.0], [-4.0, -4.0, -4.0, 0.0], [2.0, 4.0, 2.0, -2.0], [4.0, 2.0, 4.0, 4.0]], 'layers.1.weight': [[-2.0, 2.0, -2.0, 2.0, -2.0], [0.0, 0.0, 0.0, 0.0, 2.0]]}\n",
            "26 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.8125\n",
            "Epoch 500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.9375\n",
            "Epoch 2500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 4000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "0.9375\n",
            "1.0\n",
            "stage:  2\n",
            "1.0\n",
            "stage:  3\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[0.0, -2.0, -2.0, -2.0], [4.0, 4.0, 4.0, 4.0], [-2.0, -2.0, 2.0, -2.0], [2.0, 2.0, 2.0, 0.0], [0.0, 0.0, 2.0, 0.0]], 'layers.1.weight': [[0.0, -2.0, -2.0, 2.0, 2.0], [0.0, 2.0, 0.0, 0.0, 0.0]]}\n",
            "27 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.75\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.8125\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.9375\n",
            "Epoch 2000, Loss: 31.2500, Accuracy: 0.6875\n",
            "1.0\n",
            "stage:  1\n",
            "0.5625\n",
            "1.0\n",
            "stage:  2\n",
            "0.5625\n",
            "0.8125\n",
            "Epoch 2500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.9375\n",
            "Epoch 3000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 3500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 10500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 13500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 14500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 15000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 18000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 19000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 19500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 20000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 24500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 34500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 36000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 47000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 55000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 79000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 79500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 81000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 81500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 97000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 97500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 102000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 102500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 114000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 114500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 119000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 123000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 123500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 124000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 124500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 131000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 131500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 133000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 133500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 136500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 138000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 138500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 151000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 151500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 154000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 154500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 165000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 165500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 169000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 169500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 178000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 178500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 189000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 189500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 190000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 190500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 198000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 198500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 200000, Loss: 6.2500, Accuracy: 0.9375\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 93.75%\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 93.75%\n",
            "28 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 38500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 46500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 55000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 62500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 64500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 67500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 73500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 79000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 81500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 87000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 88500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 93000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 97000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 99000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 100000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 102500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 106500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 112500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 114500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 122000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 123500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 124000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 129000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 130500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 131500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 132000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 133000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 135000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 136000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 138500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 139500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 142500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 144000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 145500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 146000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 148500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 150000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 151000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 156000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 157500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 158500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 159000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 160500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 161000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 162000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 163500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 165500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 166500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 168000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 169000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 171000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 172500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 174000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 175500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 177000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 178000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 180000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 182500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 184500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 187500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 188000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 189000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 189500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 190000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 192000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 193500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 195000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 196500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 198500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 199500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 200000, Loss: 6.2500, Accuracy: 0.9375\n",
            "x= tensor([[ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "x= tensor([[ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "29 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 2000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 3500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 5500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.8125\n",
            "1.0\n",
            "stage:  3\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 17500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[2.0, 2.0, 0.0, 2.0], [2.0, 2.0, 2.0, 0.0], [0.0, -2.0, -2.0, -2.0], [2.0, 4.0, 4.0, 4.0], [0.0, 0.0, 4.0, 0.0]], 'layers.1.weight': [[2.0, 2.0, 0.0, -4.0, 2.0], [0.0, 0.0, 0.0, 2.0, 0.0]]}\n",
            "30 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "Epoch 500, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 6000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  1\n",
            "0.9375\n",
            "1.0\n",
            "stage:  2\n",
            "0.9375\n",
            "1.0\n",
            "stage:  3\n",
            "0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[2.0, 2.0, 2.0, -2.0], [4.0, 2.0, -2.0, 2.0], [-2.0, -2.0, -2.0, -2.0], [4.0, 4.0, 4.0, 4.0], [0.0, 0.0, 4.0, 2.0]], 'layers.1.weight': [[2.0, 2.0, -2.0, -4.0, 2.0], [0.0, 0.0, 0.0, 2.0, 0.0]]}\n",
            "31 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 25.0000, Accuracy: 0.7500\n",
            "0.9375\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 4500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  1\n",
            "0.6875\n",
            "0.9375\n",
            "Epoch 8000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  2\n",
            "1.0\n",
            "stage:  3\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[2.0, 2.0, 2.0, 2.0], [4.0, -2.0, 4.0, 2.0], [-4.0, -4.0, -4.0, 0.0], [-2.0, -4.0, -2.0, -4.0], [0.0, 2.0, 0.0, 0.0]], 'layers.1.weight': [[-2.0, 2.0, -2.0, 0.0, 2.0], [2.0, 0.0, 0.0, -2.0, 0.0]]}\n",
            "32 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.9375\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 3000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.75\n",
            "0.8125\n",
            "0.875\n",
            "Epoch 3500, Loss: 12.5000, Accuracy: 0.8750\n",
            "0.9375\n",
            "Epoch 4000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5500, Loss: 25.0000, Accuracy: 0.7500\n",
            "1.0\n",
            "stage:  3\n",
            "0.75\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 8000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 8500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 9000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 11500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 12000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 12500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 14000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[4.0, 4.0, 4.0, 0.0], [-4.0, -4.0, -4.0, -4.0], [4.0, 4.0, 4.0, 4.0], [2.0, 2.0, -2.0, 0.0], [-2.0, 0.0, -4.0, -4.0]], 'layers.1.weight': [[2.0, 0.0, -2.0, 2.0, -2.0], [0.0, -2.0, 2.0, 0.0, 0.0]]}\n",
            "33 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.6875\n",
            "0.75\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "0.8125\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 1500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 10500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 11500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 12000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 34500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 46500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 55000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 59500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 64500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 67500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 73000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 73500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 81000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 87000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 88500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 93000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 93500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 97000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 99000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 106500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 112500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 114500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 119000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 123500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 129000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 130500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 132000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 133000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 135000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 136000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 139500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 142500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 144000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 145500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 148500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 149500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 150000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 150500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 151000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 154000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 156000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 157000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 157500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 159000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 160500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 161000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 162000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 163500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 165500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 166500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 167500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 168000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 169000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 171000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 171500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 172500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 174000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 175500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 177000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 178000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 180000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 184500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 187500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 190000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 192000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 193500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 195000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 196000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 196500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 198500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 199500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 200000, Loss: 6.2500, Accuracy: 0.9375\n",
            "x= tensor([[ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "34 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 7500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  1\n",
            "0.875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  2\n",
            "0.875\n",
            "1.0\n",
            "stage:  3\n",
            "0.875\n",
            "Epoch 8500, Loss: 12.5000, Accuracy: 0.8750\n",
            "0.9375\n",
            "Epoch 9000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 13000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 25.0000, Accuracy: 0.7500\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[1., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[0.0, 2.0, 2.0, 2.0], [4.0, -2.0, 0.0, 0.0], [-2.0, -2.0, -2.0, 0.0], [2.0, 2.0, 2.0, 0.0], [4.0, 4.0, 4.0, 4.0]], 'layers.1.weight': [[2.0, 2.0, 0.0, 2.0, -4.0], [0.0, 0.0, 0.0, 0.0, 2.0]]}\n",
            "35 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.6875\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.9375\n",
            "Epoch 3000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 3500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 4000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 5500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 6000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 8500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 10500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 12000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 13500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 16500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 18000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 19500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 21000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 22500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 24000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 25500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 26000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 26500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 27000, Loss: 25.0000, Accuracy: 0.7500\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.625\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 27500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 28000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 28500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 29000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 29500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 30000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 30500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 31000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 31500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 32000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 32500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 33000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 33500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 34000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 34500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 35000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 35500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 36000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 36500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 39000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 46500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 55500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 56500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 64500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 65000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 67000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 67500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70000, Loss: 56.2500, Accuracy: 0.4375\n",
            "Epoch 70500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 74500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 77500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 80500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 81000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 87000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 87500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 88000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 88500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 90500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 98000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 106500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 107000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 110500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 113000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 114500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 116000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 119000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 120500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 124000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 124500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 129000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 129500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 130000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 130500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 131500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 134000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 134500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 139000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 140000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 140500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 143000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 143500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 144000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 144500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 145500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 146000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 146500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 148500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 149000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 151000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 153500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 156000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 156500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 157500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 158000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 158500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 159500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 160000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 162500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 164000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 164500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 166000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 167000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 167500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 168000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 168500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 171000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 171500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 172000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 172500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 173000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 173500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 174500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 175000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 176000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 176500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 177000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 177500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 179000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 180500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 181500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 182000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 182500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 185000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 185500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 187000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 187500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 188500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 190000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 192000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 192500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 193000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 193500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 194000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 194500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 195000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 195500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 199500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 200000, Loss: 18.7500, Accuracy: 0.8125\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 93.75%\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 93.75%\n",
            "36 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 1500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 3500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 4000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  3\n",
            "0.9375\n",
            "Epoch 9500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 10000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 10500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[4.0, 4.0, 2.0, 4.0], [2.0, 2.0, 2.0, -2.0], [2.0, 0.0, 4.0, 2.0], [-4.0, -4.0, 2.0, -4.0], [-2.0, -2.0, 0.0, -2.0]], 'layers.1.weight': [[-2.0, 2.0, 2.0, -2.0, 0.0], [2.0, 0.0, 0.0, 0.0, 0.0]]}\n",
            "37 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.625\n",
            "0.6875\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 1500, Loss: 31.2500, Accuracy: 0.6875\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 2000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 16500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 23000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 28000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 28500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 29000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 38500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 39500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 40500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 41000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 42500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 47500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 49000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 49500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 50000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 52000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 53500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 56500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 57000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 57500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 60500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 66000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 66500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 68000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 77500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 78500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 80500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 81500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 84000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 84500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 87000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 89500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 91500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 92000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 92500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 93500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 95500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 99000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 102000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 104500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 106500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 107500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 110000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 110500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 113000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 113500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 116500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 119000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 121500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 123500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 125000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 125500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 126500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 127000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 129000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 129500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 130500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 131500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 132000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 132500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 133500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 134000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 134500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 135000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 135500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 136500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 137000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 137500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 138000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 138500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 139000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 139500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 140000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 140500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 141500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 142000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 142500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 143000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 143500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 144000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 144500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 145000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 145500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 146500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 147000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 147500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 148000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 148500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 149500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 150000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 150500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 151500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 152000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 152500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 153500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 154500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 155000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 155500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 156000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 156500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 157500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 158500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 159000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 159500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 160000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 160500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 161000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 161500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 162000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 162500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 163500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 164000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 164500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 165500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 166000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 166500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 167000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 167500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 168500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 169000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 169500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 170500, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 171000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 171500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 172000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 172500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 173000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 173500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 174000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 174500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 175500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 176000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 176500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 177000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 177500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 178000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 178500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 179000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 179500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 180500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 181500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 182000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 182500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 183000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 183500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 184000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 184500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 185000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 185500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 186000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 186500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 187000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 187500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 188000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 188500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 189500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 190500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 191000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 191500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 192500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 193500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 194000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 194500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 195500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 196000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 196500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 197000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 197500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 198500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 199000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 199500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 200000, Loss: 18.7500, Accuracy: 0.8125\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "x= tensor([[ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 81.25%\n",
            "38 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 31.2500, Accuracy: 0.6875\n",
            "1.0\n",
            "stage:  1\n",
            "0.5\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  2\n",
            "0.875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  3\n",
            "0.875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[2.0, -2.0, -2.0, -2.0], [2.0, 0.0, 0.0, 0.0], [2.0, 2.0, 2.0, 0.0], [4.0, 4.0, 4.0, 4.0], [-2.0, 0.0, 0.0, 0.0]], 'layers.1.weight': [[-2.0, 2.0, 2.0, -2.0, 0.0], [0.0, 0.0, 0.0, 2.0, 0.0]]}\n",
            "39 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "Epoch 500, Loss: 31.2500, Accuracy: 0.6875\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 8500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 10000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "Epoch 19000, Loss: 0.0000, Accuracy: 1.0000\n",
            "1.0\n",
            "stage:  3\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 23500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 25000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 26000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 26500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 27000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 28500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 29000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 29500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 30000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 34000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 35000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 35500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 36000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 37000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 37500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 40000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[0.0, 4.0, -2.0, -2.0], [4.0, 4.0, 4.0, 0.0], [4.0, 4.0, 4.0, 4.0], [-2.0, -2.0, -2.0, -4.0], [-2.0, 0.0, -2.0, -4.0]], 'layers.1.weight': [[2.0, 4.0, -4.0, 0.0, -2.0], [0.0, 2.0, 4.0, 0.0, -2.0]]}\n",
            "40 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "0.75\n",
            "Epoch 500, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 2000, Loss: 43.7500, Accuracy: 0.5625\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.6875\n",
            "0.9375\n",
            "1.0\n",
            "stage:  3\n",
            "0.75\n",
            "0.8125\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "0.9375\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 5500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[4.0, 4.0, 2.0, 2.0], [2.0, 2.0, 2.0, 0.0], [2.0, 2.0, 2.0, 4.0], [-4.0, -4.0, -4.0, -4.0], [2.0, 2.0, 2.0, 2.0]], 'layers.1.weight': [[0.0, 2.0, 2.0, 0.0, -2.0], [0.0, 0.0, 2.0, -2.0, 2.0]]}\n",
            "41 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "Epoch 500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.75\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 1500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 2500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 3000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 8500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 9500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 14000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 14500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 17500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 21000, Loss: 6.2500, Accuracy: 0.9375\n",
            "1.0\n",
            "stage:  1\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 21500, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  2\n",
            "0.9375\n",
            "1.0\n",
            "stage:  3\n",
            "0.9375\n",
            "Epoch 22000, Loss: 37.5000, Accuracy: 0.6250\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.]])\n",
            "y= tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 1.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[2.0, 2.0, 2.0, 2.0], [-4.0, -2.0, -4.0, -4.0], [-4.0, 4.0, -4.0, -2.0], [-2.0, 4.0, -2.0, -4.0], [2.0, 4.0, 2.0, 0.0]], 'layers.1.weight': [[-2.0, 0.0, -2.0, 2.0, 2.0], [2.0, -2.0, 2.0, 0.0, 0.0]]}\n",
            "42 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "0.625\n",
            "0.6875\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1500, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 2000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 2500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 3000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 3500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 4000, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 4500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 9000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 10500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 11500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 12000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 12500, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 13000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 13500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 14000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 15000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 16000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 12.5000, Accuracy: 0.8750\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.6875\n",
            "0.8125\n",
            "1.0\n",
            "stage:  3\n",
            "0.8125\n",
            "0.875\n",
            "0.9375\n",
            "Epoch 17500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 18000, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 18500, Loss: 18.7500, Accuracy: 0.8125\n",
            "1.0\n",
            "stage:  4\n",
            "x= tensor([[ 1.,  0.,  0., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 0.,  0.,  0., -1.]])\n",
            "y= tensor([[1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "x= tensor([[ 1.,  1.,  0., -1.],\n",
            "        [ 0.,  0.,  0., -1.],\n",
            "        [ 0.,  0.,  1., -1.],\n",
            "        [ 1.,  1.,  1., -1.],\n",
            "        [ 0.,  1.,  1., -1.],\n",
            "        [ 1.,  0.,  1., -1.],\n",
            "        [ 0.,  1.,  0., -1.],\n",
            "        [ 1.,  0.,  0., -1.]])\n",
            "y= tensor([[0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]])\n",
            "-----------\n",
            "Model Accuracy: 100.0%\n",
            "{'layers.0.weight': [[4.0, -4.0, -4.0, -2.0], [2.0, 4.0, 4.0, 4.0], [-4.0, -4.0, -4.0, -2.0], [4.0, 0.0, 0.0, -2.0], [-4.0, 0.0, 0.0, -2.0]], 'layers.1.weight': [[-2.0, -2.0, -2.0, 4.0, -2.0], [0.0, 2.0, -2.0, 0.0, 0.0]]}\n",
            "43 th Iteration\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   QuantizedLinear-1                    [-1, 5]              20\n",
            "           Sigmoid-2                    [-1, 5]               0\n",
            "   QuantizedLinear-3                    [-1, 2]              10\n",
            "================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "Start LR: 0.01\n",
            "Start in stage 0\n",
            "0.5\n",
            "0.5625\n",
            "Epoch 500, Loss: 50.0000, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 50.0000, Accuracy: 0.5000\n",
            "0.625\n",
            "0.6875\n",
            "0.8125\n",
            "Epoch 1500, Loss: 31.2500, Accuracy: 0.6875\n",
            "Epoch 2000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 2500, Loss: 43.7500, Accuracy: 0.5625\n",
            "0.875\n",
            "Epoch 3000, Loss: 25.0000, Accuracy: 0.7500\n",
            "0.9375\n",
            "Epoch 3500, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 4000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 4500, Loss: 12.5000, Accuracy: 0.8750\n",
            "Epoch 5000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 5500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 6000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 6500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 7000, Loss: 43.7500, Accuracy: 0.5625\n",
            "Epoch 7500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 8000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 8500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 9000, Loss: 25.0000, Accuracy: 0.7500\n",
            "1.0\n",
            "stage:  1\n",
            "1.0\n",
            "stage:  2\n",
            "0.8125\n",
            "0.9375\n",
            "Epoch 9500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 10500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 11500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 12500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 13500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 14000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 14500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 15000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 15500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 16500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 17000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 17500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 18500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 19000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 19500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 20000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 20500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 21000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 21500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 22000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 22500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 23500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 24000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 24500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 25000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 25500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 26000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 26500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 27000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 27500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 28500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 29000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 29500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 30500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 31500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 32500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 33000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 33500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 34500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 35000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 35500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 36000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 36500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 37500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 38500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 39500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 40500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 41500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 42500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 43000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 43500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 44000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 44500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 45000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 45500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 46000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 46500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 47500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 48000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 48500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 49000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 49500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 50000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 50500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 51000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 51500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 52500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 53000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 53500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 54000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 54500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 55000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 55500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 56500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 57000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 57500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 58000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 58500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 59500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 60500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 61000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 61500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 62500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 63000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 63500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 64500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 65500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 66000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 66500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 67000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 67500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 68000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 68500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 69000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 69500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 70000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 70500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 71000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 71500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 72500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 73000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 73500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 74500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 75000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 75500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 76500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 77000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 77500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 78500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 79500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 80000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 80500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 81500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 82000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 82500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 83000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 83500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 84500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 85000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 85500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 86000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 86500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 87500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 88000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 88500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 89500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 90500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 91500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 92000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 92500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 93500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 94000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 94500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 95500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 96000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 96500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 97500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 98500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 99000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 99500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 100000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 100500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 101000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 101500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 102000, Loss: 37.5000, Accuracy: 0.6250\n",
            "Epoch 102500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 103000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 103500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 104000, Loss: 25.0000, Accuracy: 0.7500\n",
            "Epoch 104500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 105000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 105500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 106000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 106500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 107000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 107500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 108000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 108500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 109000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 109500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 110000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 110500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 111500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 112500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 113000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 113500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 114000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 114500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 115500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 116000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 116500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 117000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 117500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 118000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 118500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 119000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 119500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 120500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 121500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 122500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 123500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 124500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 125000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 125500, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 126500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127000, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 127500, Loss: 6.2500, Accuracy: 0.9375\n",
            "Epoch 128000, Loss: 18.7500, Accuracy: 0.8125\n",
            "Epoch 128500, Loss: 6.2500, Accuracy: 0.9375\n"
          ]
        }
      ],
      "source": [
        "now = datetime.now()\n",
        "start_time = now.strftime(\"%Y_%m_%d_%H_%M\")\n",
        "best_accuracy=0\n",
        "weights=list()\n",
        "for i in range(0,100):\n",
        "  print(i,\"th Iteration\")\n",
        "  possible_weights=[-4,-2,0,2,4] # Weights to use in the model\n",
        "  units=[5] # The number of elements in units list represent th number of hidden layers and the value the units of each layer. [5,5] means two hidden layers of 5 units each.\n",
        "  weights_path=None # If you want to load some pretrained weights\n",
        "  model = MLP(in_feat=n_inputs, num_classes=n_outs,units=units, s=1.0, STEP=False).to(device) # Initialize with STEP== False. It means without step function as activation\n",
        "  if weights_path is not None:\n",
        "    model.load_state_dict(torch.load(weights_path))\n",
        "  summary(model, (tuple([n_inputs])))\n",
        "\n",
        "  lr=0.01 # Default\n",
        "  stage=0\n",
        "  max_epochs=200000 # Default\n",
        "  T_max=10000 # Epochs for cosine_scheduler\n",
        "  warmup_epochs=2000 # 0 if no warmup. I used 2000 and it worked fine.\n",
        "  base_lr=0.0 # initial learning rate\n",
        "\n",
        "  ##################################################################################\n",
        "  ##################################################################################\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  loss_function = nn.BCELoss()\n",
        "\n",
        "  cosine_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
        "  warmup_scheduler = WarmupScheduler(\n",
        "      optimizer,\n",
        "      warmup_epochs=warmup_epochs,\n",
        "      base_lr=0.0,  # Starting from 0\n",
        "      final_lr=lr,  # Target learning rate after warmup\n",
        "      after_scheduler=cosine_scheduler\n",
        "  )\n",
        "\n",
        "  print(f\"Start LR: {lr}\")\n",
        "  print(f\"Start in stage {stage}\")\n",
        "  for epoch in range(max_epochs):  # Number of epochs\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      total_accuracy = 0\n",
        "      if stage==0:\n",
        "        if epoch%3==0:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=True\n",
        "          model.STEP=True\n",
        "        else:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=False\n",
        "          model.STEP=True\n",
        "      if stage==1:\n",
        "        if epoch%2==0:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=True\n",
        "          model.STEP=True\n",
        "        else:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=False\n",
        "          model.STEP=True\n",
        "      if stage==2:\n",
        "        if epoch%5==0:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=False\n",
        "          model.STEP=True\n",
        "        else:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=True\n",
        "          model.STEP=True\n",
        "      if stage==3:\n",
        "          for layer in model.layers:\n",
        "            layer.PTQ=True\n",
        "          model.STEP=True\n",
        "      for x, y in train_loader:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(x)\n",
        "          total_accuracy += calculate_accuracy(y, output).item()\n",
        "\n",
        "          if (total_accuracy/len(train_loader))>best_accuracy:\n",
        "            best_accuracy=(total_accuracy/len(train_loader))\n",
        "            print(best_accuracy)\n",
        "            torch.save(model.state_dict(), f'{Nbits}bits_best_model_{start_time}.pth')\n",
        "          if (total_accuracy / len(train_loader) == 1.0 and stage==3) or (total_accuracy / len(train_loader) == 1.0 and stage<3):\n",
        "            print(\"stage: \",stage+1)\n",
        "            torch.save(model.state_dict(), f'{Nbits}bits_stage_model_{start_time}.pth')\n",
        "            best_accuracy=0\n",
        "            stage=stage+1\n",
        "            break\n",
        "\n",
        "          loss = loss_function(output, y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          clip_weights(model, min(possible_weights)-1, max(possible_weights)+1)\n",
        "          total_loss += loss.item()\n",
        "      warmup_scheduler.step()\n",
        "      if total_accuracy / len(train_loader) == 1 and stage==4:\n",
        "            break\n",
        "      if (epoch+1) % 500 == 0:\n",
        "          average_loss = total_loss / len(train_loader)\n",
        "          average_accuracy = total_accuracy / len(train_loader)\n",
        "          print(f'Epoch {epoch+1}, Loss: {average_loss:.4f}, Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "  quantize_weights(model)\n",
        "\n",
        "  #### Save model results #################################################\n",
        "  model_dict={\n",
        "      \"Nbits\":Nbits,\n",
        "      \"Batch_size\":batch_size,\n",
        "      \"Add_constant\":add_constant,\n",
        "      \"Constant_value\":constant_value if add_constant else None,\n",
        "      \"n_inputs\":n_inputs,\n",
        "      \"n_outputs\":n_outs,\n",
        "      \"possible_weights\": possible_weights, # Weights to use in the model\n",
        "      \"units\":units,\n",
        "      \"total_accuracy\": evaluate_model(train_loader),\n",
        "      \"quantized_weights\": get_weights(model)\n",
        "  }\n",
        "\n",
        "  with open(f'{Nbits}bits_training_{start_time}.json', 'w') as json_file:\n",
        "      json.dump(model_dict, json_file, indent=4)\n",
        "  if get_weights(model) not in weights:\n",
        "    if evaluate_model(train_loader)==1:\n",
        "      weights.append(get_weights(model))\n",
        "      print(get_weights(model))\n",
        "      save_model(model, f'/content/drive/MyDrive/one_bit_NAT_{i}.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "id": "SjD0RY0wOoQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}